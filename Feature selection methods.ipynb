{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etww59LhvAkN",
        "outputId": "94216f07-2153-42bc-8937-008155783286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.12/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2025.8.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "n-eYMnnivOhX"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "def get_top_mutual_info_features(X, y, top_k=5, discrete_features='auto'):\n",
        "    \"\"\"\n",
        "    Calculates mutual information scores for all features in X with respect to the target y.\n",
        "\n",
        "    Parameters:\n",
        "    - X (pd.DataFrame): Feature dataset\n",
        "    - y (array-like): Target variable (class labels)\n",
        "    - top_k (int): Number of top features to return based on MI scores\n",
        "    - discrete_features (bool, array-like, or ‘auto’): Whether to consider features as discrete\n",
        "\n",
        "    Returns:\n",
        "    - mi_scores (np.ndarray): Array of mutual information scores\n",
        "    - mi_series (pd.Series): Series of MI scores labeled by feature names, sorted descending\n",
        "    - top_features (pd.Index): Index of top_k features with highest MI scores\n",
        "    \"\"\"\n",
        "    # Calculate mutual information scores\n",
        "    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\n",
        "\n",
        "    # Create a pandas Series for labeled, sorted scores\n",
        "    mi_series = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "    # Select top_k features\n",
        "    top_features = mi_series.index[:top_k]\n",
        "\n",
        "    # Output (optional prints can be added if needed)\n",
        "    return top_features\n"
      ],
      "metadata": {
        "id": "LvVqRQRWw8-k"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import f_classif\n",
        "'''\n",
        "f_score -->>> how well each feature separates the classes.\n",
        "A p-value is the probability of observing the data (or something more extreme) if the null hypothesis is true.\n",
        "Probability that how much a feature contributing in identifying that feature is useful or not. Higher the p_value,lesser will the significance pf that feature. We can understand it like opposite as in definition abpve'''\n",
        "def get_top_fisher_score_features(X,y,top_k=5):\n",
        "  f_score,f_values=f_classif(X,y)\n",
        "  # print(f_score,f_values)\n",
        "\n",
        "  fisher_score=pd.Series(f_score,index=X.columns).sort_values(ascending=False)\n",
        "  # print(\"Fisher Score\\n\\n\",fisher_score)\n",
        "  top_fisher_features=fisher_score.index[:top_k]\n",
        "  return top_fisher_features\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kaTTrOWSzPsy"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install skrebate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2Nv9DKSzYBR",
        "outputId": "31c0ee6b-2b72-43b0-f796-c36a1f9c60dd"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: skrebate in /usr/local/lib/python3.12/dist-packages (0.62)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from skrebate) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from skrebate) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from skrebate) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->skrebate) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->skrebate) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pearson correlation Feature selection method\n",
        "def get_top_pearson_features(X,y,top_k=5):\n",
        "  correlation=X.corrwith(y).abs().sort_values(ascending=False)\n",
        "  top_corr_features=correlation.index[:top_k]\n",
        "  return top_corr_features"
      ],
      "metadata": {
        "id": "2NCYJWo_zQWZ"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Relief correlaton Feature selection method\n",
        "from skrebate import ReliefF\n",
        "\n",
        "def get_top_relief_feature(X,y,top_k=5):\n",
        "  relief=ReliefF(n_neighbors=100,n_features_to_select=X.shape[1])\n",
        "  relief.fit(X.values,y.values)\n",
        "  relief_score=pd.Series(relief.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "  top_relief_features=relief_score.index[:top_k]\n",
        "  return top_relief_features"
      ],
      "metadata": {
        "id": "Vb4qeQ6O46ns"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifiers = {\n",
        "    \"GaussianNB\": GaussianNB(),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
        "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
        "    \"SVM\": SVC(max_iter=5000, random_state=42),  # increased max_iter\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=5000, random_state=42)  # increased max_iter\n",
        "}\n"
      ],
      "metadata": {
        "id": "JvSpPghmArdk"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to evaluate accuracy,precision,recall and f1-score using above classifiers and then using voting classifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_with_voting(X, y, classifiers, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Evaluates multiple classifiers + a VotingClassifier on given dataset.\n",
        "\n",
        "    Parameters:\n",
        "        X (pd.DataFrame): Features\n",
        "        y (pd.Series): Target labels\n",
        "        classifiers (dict): Dictionary of classifiers\n",
        "        test_size (float): Test split ratio\n",
        "        random_state (int): Random seed\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Results with accuracy, precision, recall, f1-score\n",
        "    \"\"\"\n",
        "    # Split into train/test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # --- Evaluate individual classifiers ---\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        results.append({\n",
        "            \"Classifier\": clf_name,\n",
        "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "            \"Precision\": precision_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
        "            \"Recall\": recall_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
        "            \"F1_Score\": f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
        "        })\n",
        "\n",
        "    # --- Voting Classifier (combine all models) ---\n",
        "    voting_clf = VotingClassifier(\n",
        "        estimators=[(name, model) for name, model in classifiers.items()],\n",
        "        voting='hard'   # could also try 'soft' if probabilities supported\n",
        "    )\n",
        "    voting_clf.fit(X_train, y_train)\n",
        "    y_pred_voting = voting_clf.predict(X_test)\n",
        "\n",
        "    results.append({\n",
        "        \"Classifier\": \"VotingClassifier\",\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred_voting),\n",
        "        \"Precision\": precision_score(y_test, y_pred_voting, average=\"weighted\", zero_division=0),\n",
        "        \"Recall\": recall_score(y_test, y_pred_voting, average=\"weighted\", zero_division=0),\n",
        "        \"F1_Score\": f1_score(y_test, y_pred_voting, average=\"weighted\", zero_division=0)\n",
        "    })\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "8iaXILFzAsst"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wine Dataset\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine=load_wine()\n",
        "\n",
        "df=pd.DataFrame(wine.data,columns=wine.feature_names)\n",
        "df[\"target\"]=wine.target\n",
        "\n",
        "X=df.drop(['target'],axis=1)\n",
        "X\n",
        "wine_y=df['target']\n",
        "y\n",
        "\n",
        "original_columns=X.columns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scalar=StandardScaler()\n",
        "X=scalar.fit_transform(X)\n",
        "X\n",
        "\n",
        "X=pd.DataFrame(X,columns=original_columns)\n",
        "X\n",
        "\n",
        "#Feature selection from different methods\n",
        "wine_main_feature=get_top_mutual_info_features(X,wine_y,5)\n",
        "wine_fisher_feature=get_top_fisher_score_features(X,wine_y,5)\n",
        "wine_pearson_feature=get_top_pearson_features(X,wine_y,5)\n",
        "wine_relief_feature=get_top_relief_feature(X,wine_y,5)\n",
        "\n",
        "wine_main_feature_data=X[wine_main_feature]\n",
        "wine_fisher_feature_data=X[wine_fisher_feature]\n",
        "wine_pearson_feature_data=X[wine_pearson_feature]\n",
        "wine_relief_feature_data=X[wine_relief_feature]\n"
      ],
      "metadata": {
        "id": "D82iqiQyvQRj"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Breast Cancer Dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "df[\"target\"] = cancer.target  # 0 = malignant, 1 = benign\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Target distribution:\\n\", df[\"target\"].value_counts())\n",
        "\n",
        "# Split into features (X) and target (y)\n",
        "X = df.drop(\"target\", axis=1)\n",
        "cancer_y = df[\"target\"]\n",
        "\n",
        "# Standardize features (important for SVM, Logistic Regression, KNN, Relief, etc.)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert back to DataFrame with column names\n",
        "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "cancer_mutual_feature=get_top_mutual_info_features(X,cancer_y,8)\n",
        "cancer_fisher_feature=get_top_fisher_score_features(X,cancer_y,8)\n",
        "cancer_pearson_feature=get_top_pearson_features(X,cancer_y,8)\n",
        "cancer_relief_feature=get_top_relief_feature(X,cancer_y,8)\n",
        "\n",
        "cancer_mutual_feature_data=X[cancer_mutual_feature]\n",
        "cancer_fisher_feature_data=X[cancer_fisher_feature]\n",
        "cancer_pearson_feature_data=X[cancer_pearson_feature]\n",
        "cancer_relief_feature_data=X[cancer_relief_feature]"
      ],
      "metadata": {
        "id": "FeNORc37vpeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39fc7062-cdb4-43ac-ca23-dd439d7746bb"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (569, 31)\n",
            "Target distribution:\n",
            " target\n",
            "1    357\n",
            "0    212\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Spambase Dataset\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "spambase=fetch_ucirepo(id=94)\n",
        "spambase\n",
        "\n",
        "featuresData=spambase.data.features\n",
        "targetData=spambase.data.targets\n",
        "featuresData\n",
        "\n",
        "spambase_full=pd.concat([featuresData,targetData],axis=1)\n",
        "spambase_full.columns\n",
        "\n",
        "X=spambase_full.drop(['Class'],axis=1)\n",
        "spambase_y=spambase_full['Class']\n",
        "\n",
        "original_columns=X.columns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scalar=StandardScaler()\n",
        "X=scalar.fit_transform(X)\n",
        "X\n",
        "\n",
        "X=pd.DataFrame(X,columns=original_columns)\n",
        "X\n",
        "\n",
        "spambase_main_feature=get_top_mutual_info_features(X,spambase_y,5)\n",
        "spambase_fisher_feature=get_top_fisher_score_features(X,spambase_y,5)\n",
        "spambase_pearson_feature=get_top_pearson_features(X,spambase_y,5)\n",
        "spambase_relief_feature=get_top_relief_feature(X,spambase_y,5)\n",
        "\n",
        "spambase_main_feature_data=X[spambase_main_feature]\n",
        "spambase_fisher_feature_data=X[spambase_fisher_feature]\n",
        "spambase_pearson_feature_data=X[spambase_pearson_feature]\n",
        "spambase_relief_feature_data=X[spambase_relief_feature]"
      ],
      "metadata": {
        "id": "9hHCI648oFzz"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Breast cancer_main_feature_data\n",
        "X_train,X_test,y_train,y_test=train_test_split(cancer_mutual_feature_data,cancer_y,test_size=0.2,random_state=42)\n",
        "cancer_results = evaluate_with_voting(cancer_mutual_feature_data, cancer_y, classifiers)\n",
        "print(\"For Cancer Dataset - Mutual Information: \\n\\n\",cancer_results)\n",
        "#For Breast cancer_fisher_feature_data\n",
        "X_train,X_test,y_train,y_test=train_test_split(cancer_fisher_feature_data,cancer_y,test_size=0.2,random_state=42)\n",
        "cancer_results = evaluate_with_voting(cancer_fisher_feature_data, cancer_y, classifiers)\n",
        "print(\"For Cancer Dataset - Fisher Score: \\n\\n\",cancer_results)\n",
        "#For Breast cancer_pearson_feature_data\n",
        "X_train,X_test,y_train,y_test=train_test_split(cancer_pearson_feature_data,cancer_y,test_size=0.2,random_state=42)\n",
        "cancer_results = evaluate_with_voting(cancer_pearson_feature_data, cancer_y, classifiers)\n",
        "print(\"For Cancer Dataset - Pearson Correlation : \\n\\n\",cancer_results)\n",
        "#For Breast cancer_relief_feature_data\n",
        "X_train,X_test,y_train,y_test=train_test_split(cancer_relief_feature_data,cancer_y,test_size=0.2,random_state=42)\n",
        "cancer_results = evaluate_with_voting(cancer_relief_feature_data, cancer_y, classifiers)\n",
        "print(\"For Cancer Dataset - Relief : \\n\\n\",cancer_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WAh9Vb6Ip6B",
        "outputId": "7803feda-d613-4577-f595-5fb5892d5c30"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Cancer Dataset - Mutual Information: \n",
            "\n",
            "            Classifier  Accuracy  Precision    Recall  F1_Score\n",
            "0          GaussianNB  0.938596   0.939042  0.938596  0.938743\n",
            "1        DecisionTree  0.912281   0.913671  0.912281  0.912683\n",
            "2    GradientBoosting  0.947368   0.947440  0.947368  0.947087\n",
            "3        RandomForest  0.964912   0.964912  0.964912  0.964912\n",
            "4                 SVM  0.964912   0.965185  0.964912  0.964725\n",
            "5                 KNN  0.947368   0.947368  0.947368  0.947368\n",
            "6  LogisticRegression  0.947368   0.948462  0.947368  0.947610\n",
            "7    VotingClassifier  0.947368   0.947368  0.947368  0.947368\n",
            "For Cancer Dataset - Fisher Score: \n",
            "\n",
            "            Classifier  Accuracy  Precision    Recall  F1_Score\n",
            "0          GaussianNB  0.929825   0.931066  0.929825  0.930146\n",
            "1        DecisionTree  0.921053   0.923417  0.921053  0.921574\n",
            "2    GradientBoosting  0.956140   0.956073  0.956140  0.956027\n",
            "3        RandomForest  0.947368   0.947368  0.947368  0.947368\n",
            "4                 SVM  0.956140   0.956505  0.956140  0.956245\n",
            "5                 KNN  0.964912   0.964912  0.964912  0.964912\n",
            "6  LogisticRegression  0.938596   0.940758  0.938596  0.939002\n",
            "7    VotingClassifier  0.956140   0.956505  0.956140  0.956245\n",
            "For Cancer Dataset - Pearson Correlation : \n",
            "\n",
            "            Classifier  Accuracy  Precision    Recall  F1_Score\n",
            "0          GaussianNB  0.929825   0.931066  0.929825  0.930146\n",
            "1        DecisionTree  0.921053   0.923417  0.921053  0.921574\n",
            "2    GradientBoosting  0.956140   0.956073  0.956140  0.956027\n",
            "3        RandomForest  0.947368   0.947368  0.947368  0.947368\n",
            "4                 SVM  0.956140   0.956505  0.956140  0.956245\n",
            "5                 KNN  0.964912   0.964912  0.964912  0.964912\n",
            "6  LogisticRegression  0.938596   0.940758  0.938596  0.939002\n",
            "7    VotingClassifier  0.956140   0.956505  0.956140  0.956245\n",
            "For Cancer Dataset - Relief : \n",
            "\n",
            "            Classifier  Accuracy  Precision    Recall  F1_Score\n",
            "0          GaussianNB  0.938596   0.939042  0.938596  0.938743\n",
            "1        DecisionTree  0.929825   0.929825  0.929825  0.929825\n",
            "2    GradientBoosting  0.947368   0.947440  0.947368  0.947087\n",
            "3        RandomForest  0.938596   0.938435  0.938596  0.938438\n",
            "4                 SVM  0.964912   0.965185  0.964912  0.964725\n",
            "5                 KNN  0.947368   0.947368  0.947368  0.947368\n",
            "6  LogisticRegression  0.947368   0.948462  0.947368  0.947610\n",
            "7    VotingClassifier  0.947368   0.947368  0.947368  0.947368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For wine_main_feature_data\n",
        "X_train,X_test,y_train,y_test=train_test_split(wine_main_feature_data,wine_y,test_size=0.2,random_state=42)\n",
        "wine_results = evaluate_with_voting(wine_main_feature_data, wine_y, classifiers)\n",
        "print(\"For Wine Dataset - Mutual Information : \\n\\n\",wine_results)\n",
        "#For wine_fisher_feature_data\n",
        "X_train,X_test,y_train,y_test=train_test_split(wine_fisher_feature_data,wine_y,test_size=0.2,random_state=42)\n",
        "wine_results = evaluate_with_voting(wine_fisher_feature_data, wine_y, classifiers)\n",
        "print(\"For Wine Dataset - Fisher Score: \\n\\n\",wine_results)\n",
        "#For wine_pearson_feature_data\n",
        "X_train,X_test,y_train,y_test=train_test_split(wine_pearson_feature_data,wine_y,test_size=0.2,random_state=42)\n",
        "wine_results = evaluate_with_voting(wine_pearson_feature_data, wine_y, classifiers)\n",
        "print(\"For Wine Dataset - Pearson Correlation: \\n\\n\",wine_results)\n",
        "#For wine_relief_feature_data\n",
        "X_train,X_test,y_train,y_test=train_test_split(wine_relief_feature_data,wine_y,test_size=0.2,random_state=42)\n",
        "wine_results = evaluate_with_voting(wine_relief_feature_data, wine_y, classifiers)\n",
        "print(\"For Wine Dataset - Relief: \\n\\n\",wine_results)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqQWYs0B2lDI",
        "outputId": "979ebe30-0e4c-43c9-98b4-acecff81a21d"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Wine Dataset - Mutual Information : \n",
            "\n",
            "            Classifier  Accuracy  Precision    Recall  F1_Score\n",
            "0          GaussianNB  0.944444   0.949106  0.944444  0.943525\n",
            "1        DecisionTree  1.000000   1.000000  1.000000  1.000000\n",
            "2    GradientBoosting  1.000000   1.000000  1.000000  1.000000\n",
            "3        RandomForest  1.000000   1.000000  1.000000  1.000000\n",
            "4                 SVM  0.972222   0.974359  0.972222  0.972263\n",
            "5                 KNN  0.972222   0.974359  0.972222  0.972263\n",
            "6  LogisticRegression  1.000000   1.000000  1.000000  1.000000\n",
            "7    VotingClassifier  1.000000   1.000000  1.000000  1.000000\n",
            "For Wine Dataset - Fisher Score: \n",
            "\n",
            "            Classifier  Accuracy  Precision    Recall  F1_Score\n",
            "0          GaussianNB  0.944444   0.949106  0.944444  0.943525\n",
            "1        DecisionTree  1.000000   1.000000  1.000000  1.000000\n",
            "2    GradientBoosting  1.000000   1.000000  1.000000  1.000000\n",
            "3        RandomForest  0.972222   0.974359  0.972222  0.972263\n",
            "4                 SVM  0.972222   0.974359  0.972222  0.972263\n",
            "5                 KNN  0.972222   0.974359  0.972222  0.972263\n",
            "6  LogisticRegression  1.000000   1.000000  1.000000  1.000000\n",
            "7    VotingClassifier  0.972222   0.974359  0.972222  0.972263\n",
            "For Wine Dataset - Pearson Correlation: \n",
            "\n",
            "            Classifier  Accuracy  Precision    Recall  F1_Score\n",
            "0          GaussianNB  0.944444   0.952381  0.944444  0.944444\n",
            "1        DecisionTree  1.000000   1.000000  1.000000  1.000000\n",
            "2    GradientBoosting  1.000000   1.000000  1.000000  1.000000\n",
            "3        RandomForest  1.000000   1.000000  1.000000  1.000000\n",
            "4                 SVM  1.000000   1.000000  1.000000  1.000000\n",
            "5                 KNN  0.944444   0.952381  0.944444  0.944444\n",
            "6  LogisticRegression  0.944444   0.946581  0.944444  0.944269\n",
            "7    VotingClassifier  1.000000   1.000000  1.000000  1.000000\n",
            "For Wine Dataset - Relief: \n",
            "\n",
            "            Classifier  Accuracy  Precision    Recall  F1_Score\n",
            "0          GaussianNB  0.944444   0.949106  0.944444  0.943525\n",
            "1        DecisionTree  0.972222   0.974074  0.972222  0.972097\n",
            "2    GradientBoosting  1.000000   1.000000  1.000000  1.000000\n",
            "3        RandomForest  1.000000   1.000000  1.000000  1.000000\n",
            "4                 SVM  0.972222   0.974359  0.972222  0.972263\n",
            "5                 KNN  0.972222   0.974359  0.972222  0.972263\n",
            "6  LogisticRegression  1.000000   1.000000  1.000000  1.000000\n",
            "7    VotingClassifier  1.000000   1.000000  1.000000  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For spambase_main_feature_data\n",
        "X_train,X_test,y_train,y_test=train_test_split(spambase_main_feature_data,spambase_y,test_size=0.2,random_state=42)\n",
        "spambase_results = evaluate_with_voting(spambase_main_feature_data, spambase_y, classifiers)\n",
        "print(\"For Spambase Dataset - Mutual Information : \\n\\n\",spambase_results)\n",
        "# spambase_fisher_feature_data\n",
        "X_train,X_test,y_train,y_test=train_test_split(spambase_fisher_feature_data,spambase_y,test_size=0.2,random_state=42)\n",
        "spambase_results = evaluate_with_voting(spambase_fisher_feature_data, spambase_y, classifiers)\n",
        "print(\"For Spambase Dataset - Fisher Score: \\n\\n\",spambase_results)\n",
        "# spambase_pearson_feature_data\n",
        "X_train,X_test,y_train,y_test=train_test_split(spambase_pearson_feature_data,spambase_y,test_size=0.2,random_state=42)\n",
        "spambase_results = evaluate_with_voting(spambase_pearson_feature_data, spambase_y, classifiers)\n",
        "print(\"For Spambase Dataset - Pearson Correlation: \\n\\n\",spambase_results)\n",
        "# spambase_relief_feature_data\n",
        "X_train,X_test,y_train,y_test=train_test_split(spambase_relief_feature_data,spambase_y,test_size=0.2,random_state=42)\n",
        "spambase_results = evaluate_with_voting(spambase_relief_feature_data, spambase_y, classifiers)\n",
        "print(\"For Spambase Dataset - Relief: \\n\\n\",spambase_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F46Gb7lDCbtC",
        "outputId": "c4925ec2-f437-493a-dafd-357a89b55ac7"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Spambase Dataset - Mutual Information : \n",
            "\n",
            "            Classifier  Accuracy  Precision    Recall  F1_Score\n",
            "0          GaussianNB  0.751357   0.791085  0.751357  0.723713\n",
            "1        DecisionTree  0.861021   0.861021  0.861021  0.861021\n",
            "2    GradientBoosting  0.869707   0.869684  0.869707  0.868432\n",
            "3        RandomForest  0.890337   0.890429  0.890337  0.889444\n",
            "4                 SVM  0.849077   0.850383  0.849077  0.846436\n",
            "5                 KNN  0.853420   0.853074  0.853420  0.851937\n",
            "6  LogisticRegression  0.827362   0.830180  0.827362  0.823011\n",
            "7    VotingClassifier  0.878393   0.881508  0.878393  0.876118\n",
            "For Spambase Dataset - Fisher Score: \n",
            "\n",
            "            Classifier  Accuracy  Precision    Recall  F1_Score\n",
            "0          GaussianNB  0.796960   0.818970  0.796960  0.783297\n",
            "1        DecisionTree  0.832790   0.832073  0.832790  0.830926\n",
            "2    GradientBoosting  0.839305   0.840808  0.839305  0.836166\n",
            "3        RandomForest  0.843648   0.842892  0.843648  0.842325\n",
            "4                 SVM  0.832790   0.836163  0.832790  0.828501\n",
            "5                 KNN  0.833876   0.833143  0.833876  0.832081\n",
            "6  LogisticRegression  0.818675   0.824511  0.818675  0.812518\n",
            "7    VotingClassifier  0.842562   0.845664  0.842562  0.838879\n",
            "For Spambase Dataset - Pearson Correlation: \n",
            "\n",
            "            Classifier  Accuracy  Precision    Recall  F1_Score\n",
            "0          GaussianNB  0.796960   0.818970  0.796960  0.783297\n",
            "1        DecisionTree  0.832790   0.832073  0.832790  0.830926\n",
            "2    GradientBoosting  0.839305   0.840808  0.839305  0.836166\n",
            "3        RandomForest  0.843648   0.842892  0.843648  0.842325\n",
            "4                 SVM  0.832790   0.836163  0.832790  0.828501\n",
            "5                 KNN  0.833876   0.833143  0.833876  0.832081\n",
            "6  LogisticRegression  0.818675   0.824511  0.818675  0.812518\n",
            "7    VotingClassifier  0.842562   0.845664  0.842562  0.838879\n",
            "For Spambase Dataset - Relief: \n",
            "\n",
            "            Classifier  Accuracy  Precision    Recall  F1_Score\n",
            "0          GaussianNB  0.680782   0.812892  0.680782  0.671180\n",
            "1        DecisionTree  0.801303   0.799712  0.801303  0.799557\n",
            "2    GradientBoosting  0.830619   0.829620  0.830619  0.829711\n",
            "3        RandomForest  0.826276   0.825225  0.826276  0.825344\n",
            "4                 SVM  0.805646   0.804317  0.805646  0.804546\n",
            "5                 KNN  0.804560   0.805642  0.804560  0.805004\n",
            "6  LogisticRegression  0.792617   0.790962  0.792617  0.789784\n",
            "7    VotingClassifier  0.823018   0.822630  0.823018  0.822799\n"
          ]
        }
      ]
    }
  ]
}